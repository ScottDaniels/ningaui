
# ======================================================================== v1.0/0a157 
#                                                         
#       This software is part of the AT&T Ningaui distribution 
#	
#       Copyright (c) 2001-2009 by AT&T Intellectual Property. All rights reserved
#	AT&T and the AT&T logo are trademarks of AT&T Intellectual Property.
#	
#       Ningaui software is licensed under the Common Public
#       License, Version 1.0  by AT&T Intellectual Property.
#                                                                      
#       A copy of the License is available at    
#       http://www.opensource.org/licenses/cpl1.0.txt             
#                                                                      
#       Information and Software Systems Research               
#       AT&T Labs 
#       Florham Park, NJ                            
#	
#	Send questions, comments via email to ningaui_support@research.att.com.
#	
#                                                                      
# ======================================================================== 0a229


# WARNING:   using variables other thatn $NG_ROOT and $TMP on the command line 
#	could produce undesired results.  The spyglass environment is washed 
#	before it is started and thus only a minimal set of variables will 
#	expand to something.  
#------------------------------------------------------------------------------
# this is the main spyglass config file. the spyglass start script will search
# $NB_ROOT/appl/*/lib directories for files with names matching the pattern
# spyglass.*.cfg and will pass them to spyglass as it starts.  This allows
# applications to incorporate their probes in the main spyglass without having 
# to modify the main config file. 
# -----------------------------------------------------------------------------

# ------------ mailing list stuff --------------------------------
# mailing list nicknames and notify statements in a test section may use
# environment (cartulary) variables in the form &name or &{name}. This allows
# cluster specific nicknames to be created (e.g. &{NG_CLUSTER}-adm 
#
domain	&NG_DOMAIN

#digest <name> 		<recipent(s)>			<frequency(hh:mm)> [<hr:mn to start>]
digest	daily		&NG_SPYG_DAILY,cluster-admin	24:00 			00:01
digest	morning		&NG_SPYG_MORNING		24:00			06:00
digest	hourly		&NG_SPYG_HOURLY,cluster-admin	1:00

# alert nicknames -- mappings to pinkpages varaiables to allow generic config file
nickname service	&NG_SPYG_NN_SERVICE		# alerts by catigory
nickname daemon		&NG_SPYG_NN_DAEMON
nickname filer		&NG_SPYG_NN_FILER
nickname network	&NG_SPYG_NN_NETWORK
nickname logfile	&NG_SPYG_NN_LOGFILE
nickname satellite	&{NG_SPYG_NN_SATELLITE}
nickname disk		&{NG_SPYG_NN_DISK}
nickname remote		&NG_SPYG_NN_REMOTE
nickname tape		&NG_SPYG_NN_TAPE
							# general nicknames, also used on alerts
nickname cluster-admin	&NG_SPYG_NN_ADMIN		# cluster admin
nickname feeds-admin	&NG_SPYG_NN_FEEDS		# person(s) who worry about data feeds into cluster
nickname hw-admin	&NG_SPYG_NN_HWADMIN		# hardware admin
nickname flock-admin	&NG_SPYG_NN_FLOCKADMIN

# states:
# the when directive is used to allow the test to run only when the 
# current state (as told to spyglass by some outside source) is 
# one of the states listed. Valid state names are:
# 	stable	(the node is expected to be in tip top shape with everything running)
# 	busy  	(the node is under a greater than normal load)
# 	no-network (the network interface, or some network hardware is not functioning)
# 	crisis	(the node is just buggered; tests should probably not list this)
#	startup (test run once as spyglass is started)
#
# if the tst does NOT have a when clause, then 'when stable' is assumed

# ------------- test specifications ------------------------------
# keywords to a test secion MUST be tab indented, and are:
# 	command - command to execute the test; alarm condition if command exits !0
# 	desc 	  - message placed into email when alarm is sent
# 	freq 	  - hh:mm or mm -- amount of time between tests
# 	logok   - yes/no; default is yes. if no, the return to an ok state is not logged in a digest
# 	notify  - list of comma separated email addresses or nicknames
# 	retry n [s] - number of times we retry after an alert status received before we actually report the alert
#		s is the delay between each retry in seconds. if not supplied freq/2 is used.
# 	scope {all|[!]attribute} - Test is executed on the node if it has the attribute, or all is supplied
# 	squelch - hh:mm or mm -- amount of time to be quiet about a bad state after an alert is set
#		if omitted, alarms are squelched until after we see a normal state from the test
# 	time hh:mm - Time to execute -- delay first execution until this time, then every freq hh:mm after that
#		mostly for once/twice a day checks at a specific time
# 	when statename - (see states above)
#
test green-light		# blocks most other tests until the node is up
	desc	greenlight is not running
	freq	0:10
	squelch reset
	scope	all
	when	stable busy no-network
	notify	nobody		# blocks others; no notification.  remote green-light tests catch us being down
	command	ng_wrapper -s
# ----------------------- startup tests ------------------------------------------
#	only need to do these things at startup for obvious reasons

test reboots
	when	startup
	desc	node seems to have been rebooted
	notify	cluster-admin,service
	command	ng_spy_ckmisc boot_check

test ng-dosu
	when	startup
	desc	ng_dosu ownership/mode is not right
	scope	!Satellite
	notify	cluster-admin
	command ng_spy_ckmisc dosu

# --------------------- Jobber checks ----------------------------------------------
test seneschal			# basic test to see if daemon is running
	after	green-light
	desc	seneschal or s_start may be down or suspended
	freq	0:15
	squelch	0:30
	scope	Jobber
	when	stable no-network
	retry	3 60
	notify	cluster-admin,daemon
	command	ng_spy_ckdaemon seneschal

# seneschal now uses dump1 interface and thus does not need the send dump process
#test rmdump						# test to see that rmdump is being sent to jobber
#	after	green-light
#	desc	repmgr dump file on Jobber is stale
#	freq	0:05
#	squelch	1:00
#	when	stable no-network
#	notify	cluster-admin,filer
#	scope	Jobber
#	retry	1 120
#	command ng_spy_ckfage -d $NG_ROOT/site/rm -a 300 -o -p "dump"

test nawab			# basic test to see if daemon is running
	after	green-light
	desc	nawab or n_start may be down
	freq	0:15
	squelch	0:30
	when	stable no-network
	scope	Jobber
	notify	cluster-admin,daemon
	retry	3 60
	command	ng_spy_ckdaemon nawab

test shunt			# basic test to see if daemon is running
	after	green-light
	desc	shunt listener may be down
	freq	0:15
	squelch	0:30
	when	stable no-network
	scope	Jobber
	notify	cluster-admin,daemon
	retry	3 60
	command	ng_spy_ckdaemon shunt

# ----------------------------- per node daemon checks ----------------------------
#   basic tests to see if the daemons are running and responding 
test woomera			
	after green-light
	desc  woomera or wstart may be down
	when	stable no-network
	freq 0:15
	squelch 0:30
	retry	3 60
	notify	cluster-admin,daemon
	command	ng_spy_ckdaemon woomera

test nnsd			# run everywhere; it does the right thing if not supposed to run on this node
	after green-light
	desc	nnsd not running/responding
	when	stable no-network
	freq 	0:15
	squelch 0:30
	retry	3 60
	notify	cluster-admin,daemon
	command	ng_spy_ckdaemon nnsd

test parrot	
	after	green-light
	desc 	parrot may be down
	freq	0:15
	when	stable no-network
	squelch 0:30
	retry	3 60
	notify	cluster-admin,daemon
	command	ng_spy_ckdaemon parrot

test dbd
	after	green-light
	desc  	rm_dbd may be down
	freq 	0:15
	when	stable no-network
	squelch 0:30
	retry	2 60
	notify	cluster-admin,daemon
	command	ng_spy_ckdaemon dbd

test narbalek	
	after	green-light
	desc	narbalek is down or not responding
	retry	2 90
	freq	0:10
	squelch	1:00
	when	stable no-network
	notify	cluster-admin,daemon
	command ng_spy_ckdaemon narbalek

test srvmgr
	after	green-light
	desc	service manger down or not responding
	retry	2 90
	freq	0:10
	squelch 1:00
	retry	1 60
	when 	stable no-network
	notify	cluster-admin,daemon
	command	ng_spy_ckdaemon srvmgr

test catman			# basic to see if daemon is up; other test to see about remote connectivity is later
	after	green-light
	desc	catman may be down 
	freq	0:15
	when	stable no-network
	squelch 0:30
	retry	2 60
	notify	cluster-admin,daemon
	scope	Catalogue
	command	ng_spy_ckdaemon catman

# ------------------------- remote services ---------------------------------------
#   for things that we should be able to reach (off node/off cluster)
#   these should NOT alarm if the remote node's greenlight is not running. in 
#   that case the status should be blocked so we can (eventually) track an 
#   unavailable status, but limit the amount of spam alarms because the node is 
#   not there/responsive.
test rmt-catalogue
	after	green-light
	desc	not able to reach catalogue service from &NG_CLUSTER
	freq	0:20
	when	stable
	squelch	reset
	retry	2 90
	notify	cluster-admin,remote
	scope	Alerter				# just from one node in each cluster
	# catalogu doesn't run for all clusters, -o causes check to be successful if ppvar does not xlate to host name 
	cmd	ng_spy_cknet service -o srv_Catalogue "ng_catalogue -g 1 -c"

	# an extra check to set some importance to a missing node; conn checks will scream about this, but this makes it louder
test tape-dup-service
	after	green-light
	desc	tape dup node is probably down
	freq	0:20
	when	stable
	squelch	reset
	retry	2 60
	notify	cluster-admin,tape
	scope	Alerter
	cmd	ng_spy_cknet service -m -f srv_TAPE_DUP_NODE "date"

# ----------------------------- logfile -------------------------------------------
test master-crits
	desc	CRIT messages found in the master log
	after	green-light
	when	stable no-network 
	freq	0:10
	logok	no
	squelch	0:00
	notify	cluster-admin,logfile
	command	ng_spy_ckmisc crit_check 599

	# this test should always return good; it sends mail on its own. if it is broken, 
	# then we will see an error and send our own email. 
test master-alerts
	desc	ALERT message check process seems broken
	after	green-light
	when	stable no-network 
	logok	no
	freq	0:15
	squelch	0:00
	notify	cluster-admin,logfile
	command	ng_spy_ckmisc alert_check 900

# ----------------------------- service -------------------------------------------
test service-change
	# sends alert if a service seems to be hosted on a new node
	desc	one or more service has moved nodes
	after	green-light
	when 	stable no-network
	logok	no
	freq	00:10
	squelch 00:00			# dont squelch as we dont want to miss a change
	notify	cluster-admin,service
	scope	Alerter
	command ng_spy_ckmisc service_locations

	# tape changers -- need to be one per type -- service manager depends on these tests
test tpchanger-STK 			# check the tpchanger state
	after	green-light
	desc	tape changer daemon for STK is down or heartbeat is stale
	scope	Tpchanger_STK
	when	stable no-network
	freq	0:02
	retry	1 60 
	squelch	3:00
	notify	cluster-admin,service
	command	ng_srv_tpchanger STK state

test tpchanger-pool-STK 			# check the tpchanger state based on response to pool cmd
	after	tpchanger-STK
	desc	tape changer daemon for STK is down or no response to pool command
	scope	Tpchanger_STK
	when	stable no-network
	freq	0:15
	retry	2 120
	squelch	3:00
	notify	cluster-admin,service
	command	ng_srv_tpchanger STK pool-state

test tpchanger-QSTAR 			# check the tpchanger state
	after	green-light
	desc	tape changer daemon for QSTAR is down or has stale heartbeat
	scope	Tpchanger_QSTAR
	when	stable no-network
	freq	0:02
	squelch	3:00
	retry	1 60
	notify	cluster-admin,service
	command	ng_srv_tpchanger QSTAR state

test tpchanger-pool-QSTAR 			# check the tpchanger state based on response to pool cmd
	after	tpchanger-QSTAR
	desc	tape changer daemon for QSTAR is down or no response to pool dump
	scope	Tpchanger_QSTAR
	when	stable no-network
	freq	0:15
	retry	2 120
	squelch	3:00
	notify	cluster-admin,service
	command	ng_srv_tpchanger STK pool-state

# ------------------------- Filer ------------------------------------------------
test repmgr			# basic test to see if daemon is running
	after green-light
	desc  repmgr or rmbt may be down or hung
	freq 0:15
	when	stable no-network
	squelch 0:30
	scope Filer
	retry	2 90
	notify	cluster-admin,filer
	command	ng_spy_ckdaemon repmgr

test mlog_frag
	desc	excessive mlog fragments registered; log_comb process may be broken
	after	green-light
	freq	2:00
	when	stable no-network
	squelch	12:00
	notify	cluster-admin,filer
	scope	Filer
	command	ng_spy_ckmisc mlog_frag 1000		# 1000 is the threshold (alarm if more than)

test flist-files					# reports on old filelists in repmgr nodes directory
	after	green-light
	desc	one or more flist files may be old
	when	stable no-network
	freq	0:15
	# squelch on change forces into silent mode until another becomes stale
	squelch	change
	scope	Filer
	notify	cluster-admin,filer
	command ng_spy_ckfage -d $NG_ROOT/site/rm/nodes -a 1800 -o -p ".*"

# --------------------- per node narbalek spcific -----------------------------------------------
test narbalek-data		# test to see if narbalek data looks sane
	after	green-light
	desc	narbalek data sanity check failed
	freq	0:10
	when	stable no-network
	squelch	1:00
	notify	flock-admin
	retry	3 60
	command ng_nar_tree_sanity -u spyglass -d -v 

# --------------------- alerter node (1/cluster) checks  --------------------------------------------------------
test narbalek-tree		# check the narbalek tree for multiple roots and disconnected nodes
	after	green-light
	desc	narbalek tree sanity check failed
	freq	0:15
	when	stable no-network
	scope	Alerter
	squelch	1:00
	notify	flock-admin,remote
	retry	3 60
	command ng_nar_tree_sanity -v -c

test time-sync
	after	green-light
	desc	clocks may be out of sync on node(s)
	freq	0:30
	when	stable
	scope	Alerter
	squelch	24:00
	notify	flock-admin
	command	ng_spy_ckmisc time

# ------------------------------- network ----------------------------------------------------
# connectivity tests are driven from every node in the cluster. Each node tests two other 
# neighbouring nodes. the -d option causes cknet to attempt to use the discard service to 
# test the reachability before using parrot.  this allows a report that the node is up even 
# if ningaui services are not running on the node. 

# we do two different connection tests.  The first (conn-some) verifies that we can get to the
# node(s).  If we can get to the node(s), then we test to see if we can get there on all of
# the interfaces.  So, conn-some will block conn-all.
test net-conn-all
	after	net-conn-some
	desc	network connectivity check failed: one or more interfaces may be down
	freq	0:10
	squelch change
	notify	cluster-admin,network
	retry	2 60
	when	stable
	command	ng_spy_cknet conn  -d -a

# point out that some interfaces are not working.
# if no interfaces respond, then neighbouring node(s) are likely down. this test blocks
# other network tests.
test net-conn-some
	after	green-light
	desc	all network connectivity checks failed: node may be down
	freq	0:10
	squelch change
	notify	cluster-admin,network
	retry	2 60
	when	stable
	command	ng_spy_cknet conn  -d 

test netif-dns
	after	green-light
	desc	cant reach netif node via &NG_CLUSTER or multiple nodes have netif IP alias
	freq	0:10
	squelch change
	notify	cluster-admin,network
	when	stable
	scope	Alerter
	# some nodes seem to take 4 minutes to bounce (slower mac nodes)
	retry	3 120
	command	ng_spy_cknet netif 

	# these are not network tests per se, but we list them here as they block 
	# until it seems that the network is up to our neighbours
	# we run this after conn-some as there must be someway to get to the host
test remote-greenlight
	after	net-conn-some
	desc	greenlight not running on remote node 
	freq	0:10
	squelch 6:00
	notify	cluster-admin,remote
	when	stable
	# several retries (90s apart) should avoid false alarms when bouncing the node
	retry	3 90
	scope	all
	command	ng_spy_cknet remote_green_light

test spyglass
	after	net-conn-some
	desc	spyglass not running/responding on neighbouring node
	freq	0:10
	squelch change
	notify	cluster-admin,remote
	when	stable
	scope	all
	retry	3 90
	command	ng_spy_cknet remote_spyglass

# ------------------------------- core file checks -------------------------------------------
# squelch on core file checks is 0 on purpose!  It forces an alarm each time there are new core
# files -- this one we do not want to go silent. We want to find three types of core files
# while ignoring other files that might have the string core embedded somewhere.  We will hit
# on:  core, core.name, core.pid, name.core and pid.core.  If the sysadmin gets more fancy 
# with how core files are named we will likely miss them as we try to avoid things like the 
# spyglass core file check history files like spy_out.core-files.4226401840. 
test core-files					# check for corefiles in NG_ROOT/site (not darwin)
	after	green-light
	desc	new core files found in NG_ROOT/site
	freq	1:00
	when	stable no-network
	logok	no
	squelch	0
	scope	!Darwin
	notify	cluster-admin,disk
	# -A causes all directories under site to be searched.
	command ng_spy_ckfage -d $NG_ROOT/site -A  -a 3600 -n -p "/core$|.*[.][cC][oO][rR][eE]$|[cC][oO][rR][eE][.].*"

test core-reminder					# daily reminder if core files were not removed
	after	green-light
	desc	old core files still exist in NG_ROOT/site
	freq	24:00
	when	stable no-network
	logok	no
	squelch	0
	scope	!Darwin
	notify	cluster-admin,disk
	command ng_spy_ckfage -d $NG_ROOT/site -A  -a 86401 -o -p "/[cC][oO][rR][eE]$|.*[.][cC][oO][rR][eE]$|[cC][oO][rR][eE][.].*"

test core-files-ningaui					# check for corefiles in NG_ROOT/site (not darwin)
	after	green-light
	desc	new core files found in ningaui home
	freq	1:00
	when	stable no-network
	logok	no
	squelch	0
	scope	!Darwin
	notify	cluster-admin
	command ng_spy_ckfage -d ~ningaui -a 3600 -n -p "/[cC][oO][rR][eE]$|.*[.][cC][oO][rR][eE]$|[cC][oO][rR][eE][.].*"

test core-reminder-ningaui					# reminder if core files are not removed
	after	green-light
	desc	old core files still exist in ningaui home
	freq	24:00
	when	stable no-network
	logok	no
	squelch	0
	scope	!Darwin
	notify	cluster-admin
	command ng_spy_ckfage -d ~ningaui -a 86401 -o -p "/[cC][oO][rR][eE]$|.*[.][cC][oO][rR][eE]$|[cC][oO][rR][eE][.].*"

test darwin-core-files					# check for corefiles in /cores (darwin only)
	after	green-light
	desc	new core files found in /cores
	freq	1:00
	when	stable no-network
	logok	no
	squelch	0
	notify	cluster-admin,disk
	scope	Darwin
	command ng_spy_ckfage -d /cores -a 3600 -n -p "/[cC][oO][rR][eE]$|.*[.][cC][oO][rR][eE]$|[cC][oO][rR][eE][.].*"

test darwin-core-reminder				# old core files daily reminder -- darwin only
	after	green-light
	desc	old core files still exist in /cores
	freq	24:00
	when	stable no-network
	logok	no
	squelch	0
	notify	cluster-admin,disk
	scope	Darwin
	command ng_spy_ckfage -d /cores -a 86401 -o -p "/[cC][oO][rR][eE]$|.*[.][cC][oO][rR][eE]$|[cC][oO][rR][eE][.].*"

# --------------------- file system full checks, broken mounts, and old file checks  -----------------------

test fsfull-satellite					# filesystem full check on satellite
	after	green-light
	desc	filesystem(s) usage over threshold
	scope	Satellite
	when	stable no-network
	freq	0:15
	squelch	1:00
	notify	cluster-admin,disk,satellite
	command	ng_spy_ckfsfull	84:/opt 84:/incoming 84:/l/ng_incoming
	
test fsfull-all					# filesystem full check on all nodes
	after	green-light
	desc	filesystem(s) usage over threshold
	scope	all
	freq	0:15
	squelch	12:00
	when	stable no-network
	notify	cluster-admin,disk
	command	ng_spy_ckfsfull	80:/tmp 80:$TMP 88:$NG_ROOT/site 100M:/pkfs

test darwin-vm				# darwin swap usage (an ls check on /var/vm)
	after	green-light
	desc	/var/vm contents exceed limit
	scope	Darwin
	freq	0:15
	squelch	change
	when	stable no-network
	notify	hw-admin,cluster-admin,disk
	command	ng_spy_ckfsfull	-l 6g:/var/vm/swapfile*

test bsd-vm-crash
	after	green-light
	desc	/var/crash/vmcore.[0-9] files exist
	scope	FreeBSD
	freq	1:00
	squelch	change
	when	stable no-network
	notify	hw-admin,cluster-admin,disk
	command	ng_spy_ckfage -s -d /var/crash -a 86400 -n -p "vmcore.[0-9]"

	# darwin seems not to have useful inode information
test inode-all					# filesystem inode full check on all nodes
	after	green-light
	desc	filesystem(s) inode usage above tolerence
	scope	!Darwin
	when	stable no-network
	freq	0:15
	squelch	12:00
	notify	cluster-admin,disk
	command	ng_spy_ckfsfull -p -I -i 90%:!/pkfs 99%:/pkfs		# / causes all local filesystems to be checked; -I to use site/spyglass_fsfull.ignore file

test disk-fail					# general test to see if anything suggests that disk(s) is/are failiing
	after	green-light
	desc	diskutil or other source indicates disks are in jeopardy
	scope	all
	freq	0:15
	squelch	change
	when	stable no-network
	notify	cluster-admin,disk
	command	ng_spy_ckdisk

test fs-mounts
	after   green-light
	desc    needed filesystem is not mounted, or equerry managed filesystem state not stable
	scope   all
	freq    0:15
	squelch 6:00
	when    stable no-network
	notify  cluster-admin,disk
	command ng_eqreq -v verify mounts

test dev-access
	after   green-light
	desc    access to one or more devices in /dev maybe blocked
	scope   all
	freq    0:20
	squelch 6:00
	when    stable no-network
	notify  cluster-admin,disk
	command ng_spy_ckdisk dev_access

#begnr
test syslog-perms
	# check to ensure that system log files can be read by group/world; 044 bits are on
	# the test ignores missing files so we can define one probe for all types of nodes
	after	green-light
	desc	permissions on syslog seem wrong
	scope	all
	freq	1:00
	squelch	change
	when 	stable
	notify	cluster-admin,disk
	command	ng_spy_ckdisk perms M044 /var/log/system.log /var/log/messages /var/adm/messages

	# to combat darwin nfs bugs
test syslog-nfs
	after 	green-light
	desc	nfs related error message found in system log
	scope	Darwin
	freq 	0:15
	squelch	12:00
	when	stable
	notify	cluster-admin,disk
	command	ng_spy_ckmisc syslog_check "nfs.server.*not.responding" 15
#endnr

test repmgr-space				# repmgr filesystem check at a cluster level
	after	green-light
	desc	replication manager used space above threshold
	freq	0:15
	squelch	5:00
	when	stable no-network
	scope	Alerter
	notify	cluster-admin,disk
	# must use wrapper and \$ to get var to expand at right time
	command	ng_wrapper ng_rm_df -A -N \${RM_FS_FULL_THRESHOLD:-90}

test old-cartulary
	after	green-light
	desc	cartulary may be old
	freq	0:30
	squelch	1:00
	retries 1 0:01
	notify	cluster-admin,disk
	when	stable no-network
	#command ng_spy_ckfage -d $NG_ROOT -a 300 -o -p 'cartulary$'
	command ng_spy_ckmisc cartulray_age 315

test cartulary-path
	after	green-light
	desc	PATH in cartulary seems bad
	freq	0:10
	squelch	1:00
	retries 1 30
	notify	cluster-admin
	when	stable no-network
	command ng_spy_ckmisc validate_path

# ----------------------------------- system (processes etc.) ----------------------------------

test excessive-procs
	after	green-light
	desc	excessive processes found for one or more users
	freq	0:15
	retries	2 90
	squelch	1:00
	notify	cluster-admin,hw-admin
	when	stable no-network
	command	ng_spy_ckmisc excessive_proc 250 ANY
